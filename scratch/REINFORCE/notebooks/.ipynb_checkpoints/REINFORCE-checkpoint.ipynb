{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fd93f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from collections import deque\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "matplotlib.use(\"agg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e436b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "738ee4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BreakoutNoFrameskip-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66bf8ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from base64 import b64encode\n",
    "\n",
    "def gray_show(img, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt\n",
    "    ax.axis('off')\n",
    "    ax.imshow(img, cmap='gray')\n",
    "\n",
    "def imshow(img, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt\n",
    "    ax.axis('off')\n",
    "    ax.imshow(img)\n",
    "\n",
    "def video_to_b64(path):\n",
    "    mp4 = open(path,'rb').read()\n",
    "    return \"data:video/mp4;base64,\" + b64encode(mp4).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b6f77b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "class ProcessFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape,):\n",
    "        super().__init__(env)\n",
    "        self.shape = shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, \n",
    "            high=1, shape=shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame.process(self, obs)\n",
    "\n",
    "    def process(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame = cv2.resize(frame, self.shape, interpolation=cv2.INTER_AREA)\n",
    "        return frame.astype(np.float32)/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e421291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"BreakoutNoFrameskip-v4\")\n",
    "env = ProcessFrame(env, (64, 64))\n",
    "gray_show(env.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1e462280",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.wrappers.FrameStack(env, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a5df7489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 1, (4, 4, 64, 64), uint8)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84909f9",
   "metadata": {},
   "source": [
    "# Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50408386",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePolicy(keras.Model):\n",
    "    def action_distribution(self, observations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            observations: torch.Tensor of shape [batch size, dim(observation space)]\n",
    "        Returns:\n",
    "            distribution: instance of a subclass of torch.distributions.Distribution\n",
    "\n",
    "        See https://pytorch.org/docs/stable/distributions.html#distribution\n",
    "\n",
    "        This is an abstract method and must be overridden by subclasses.\n",
    "        It will return an object representing the policy's conditional\n",
    "        distribution(s) given the observations. The distribution will have a\n",
    "        batch shape matching that of observations, to allow for a different\n",
    "        distribution for each observation in the batch.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, observations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            observations: np.array of shape [batch size, dim(observation space)]\n",
    "        Returns:\n",
    "            sampled_actions: np.array of shape [batch size, *shape of action]\n",
    "\n",
    "        Call self.action_distribution to get the distribution over actions,\n",
    "        then sample from that distribution. You will have to convert the\n",
    "        actions to a numpy array, via numpy(). Put the result in a variable\n",
    "        called sampled_actions (which will be returned).\n",
    "        \"\"\"\n",
    "        distribution = self.action_distribution(observations)\n",
    "        sampled_actions = distribution.sample().numpy()\n",
    "        return sampled_actions\n",
    "\n",
    "\n",
    "class CategoricalPolicy(BasePolicy):\n",
    "    def __init__(self, network):\n",
    "        super().__init__(self)\n",
    "        self.network = network\n",
    "\n",
    "    def action_distribution(self, observations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            observations: torch.Tensor of shape [batch size, dim(observation space)]\n",
    "        Returns:\n",
    "            distribution: torch.distributions.Categorical where the logits\n",
    "                are computed by self.network\n",
    "\n",
    "        See https://pytorch.org/docs/stable/distributions.html#categorical\n",
    "        \"\"\"\n",
    "        predictions = self.network(observations)\n",
    "        distribution = tfp.distributions.categorical.Categorical(probs=None, logits=predictions)\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class GaussianPolicy(BasePolicy):\n",
    "    def __init__(self, network, action_dim):\n",
    "        \"\"\"\n",
    "        After the basic initialization, you should create a nn.Parameter of\n",
    "        shape [dim(action space)] and assign it to self.log_stddev.\n",
    "        A reasonable initial value for log_stddev is 0 (corresponding to an\n",
    "        initial stddev of 1), but you are welcome to try different values.\n",
    "        \"\"\"\n",
    "        super().__init__(self)\n",
    "        self.network = network\n",
    "        self.log_stddev = self.add_weight(shape=(action_dim,), initializer='zeros', trainable=True)\n",
    "\n",
    "    def stddev(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            stddev: torch.Tensor of shape [dim(action space)]\n",
    "\n",
    "        The return value contains the standard deviations for each dimension\n",
    "        of the policy's actions. It can be computed from self.log_stddev\n",
    "        \"\"\"\n",
    "        stddev = tf.exp(self.log_stddev)\n",
    "        return stddev\n",
    "\n",
    "    def action_distribution(self, observations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            observations: torch.Tensor of shape [batch size, dim(observation space)]\n",
    "        Returns:\n",
    "            distribution: an instance of a subclass of\n",
    "                torch.distributions.Distribution representing a diagonal\n",
    "                Gaussian distribution whose mean (loc) is computed by\n",
    "                self.network and standard deviation (scale) is self.stddev()\n",
    "\n",
    "        Note: PyTorch doesn't have a diagonal Gaussian built in, but you can\n",
    "            fashion one out of\n",
    "            (a) torch.distributions.MultivariateNormal\n",
    "            or\n",
    "            (b) A combination of torch.distributions.Normal\n",
    "                             and torch.distributions.Independent\n",
    "        \"\"\"\n",
    "        locs = self.network(observations)\n",
    "        stddevs = self.stddev()\n",
    "        distribution = tfp.distributions.MultivariateNormalTriL(loc=locs, scale_tril=tf.linalg.diag(stddevs))\n",
    "        return distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39700a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PolicyGradient(object):\n",
    "    \"\"\"\n",
    "    Class for implementing a policy gradient algorithm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, config, seed, logger=None):\n",
    "        \"\"\"\n",
    "        Initialize Policy Gradient Class\n",
    "\n",
    "        Args:\n",
    "                env: an OpenAI Gym environment\n",
    "                config: class with hyperparameters\n",
    "                logger: logger instance from the logging module\n",
    "\n",
    "        You do not need to implement anything in this function. However,\n",
    "        you will need to use self.discrete, self.observation_dim,\n",
    "        self.action_dim, and self.lr in other methods.\n",
    "        \"\"\"\n",
    "        # directory for training outputs\n",
    "        if not os.path.exists(config.output_path):\n",
    "            os.makedirs(config.output_path)\n",
    "\n",
    "        # store hyperparameters\n",
    "        self.config = config\n",
    "        self.seed = seed\n",
    "\n",
    "        self.logger = logger\n",
    "        if logger is None:\n",
    "            self.logger = get_logger(config.log_path)\n",
    "        self.env = env\n",
    "        self.env.seed(self.seed)\n",
    "\n",
    "        # discrete vs continuous action space\n",
    "        self.discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "        self.observation_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = (\n",
    "            self.env.action_space.n if self.discrete else self.env.action_space.shape[0]\n",
    "        )\n",
    "\n",
    "        self.lr = self.config.learning_rate\n",
    "\n",
    "        self.init_policy()\n",
    "\n",
    "        if config.use_baseline:\n",
    "            self.baseline_network = BaselineNetwork(env, config)\n",
    "\n",
    "    def init_policy(self):\n",
    "        \"\"\"\n",
    "        Please do the following:\n",
    "        1. Create a network using build_mlp. It should map vectors of size\n",
    "           self.observation_dim to vectors of size self.action_dim, and use\n",
    "           the number of layers and layer size from self.config\n",
    "        2. If self.discrete = True (meaning that the actions are discrete, i.e.\n",
    "           from the set {0, 1, ..., N-1} where N is the number of actions),\n",
    "           instantiate a CategoricalPolicy.\n",
    "           If self.discrete = False (meaning that the actions are continuous,\n",
    "           i.e. elements of R^d where d is the dimension), instantiate a\n",
    "           GaussianPolicy. Either way, assign the policy to self.policy\n",
    "        3. Create an optimizer for the policy, with learning rate self.lr\n",
    "           Note that the policy is an instance of (a subclass of) nn.Module, so\n",
    "           you can call the parameters() method to get its parameters.\n",
    "        \"\"\"\n",
    "        #######################################################\n",
    "        #########   YOUR CODE HERE - 8-12 lines.   ############\n",
    "        self.network = build_mlp(\n",
    "            self.action_dim, self.config.n_layers, self.config.layer_size,\n",
    "            name=\"policy\"\n",
    "        )\n",
    "        if self.discrete:\n",
    "            self.policy = CategoricalPolicy(self.network)\n",
    "        else:\n",
    "            self.policy = GaussianPolicy(self.network, self.action_dim)\n",
    "\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=self.config.learning_rate)\n",
    "\n",
    "    def init_averages(self):\n",
    "        \"\"\"\n",
    "        You don't have to change or use anything here.\n",
    "        \"\"\"\n",
    "        self.avg_reward = 0.0\n",
    "        self.max_reward = 0.0\n",
    "        self.std_reward = 0.0\n",
    "        self.eval_reward = 0.0\n",
    "\n",
    "    def update_averages(self, rewards, scores_eval):\n",
    "        \"\"\"\n",
    "        Update the averages.\n",
    "        You don't have to change or use anything here.\n",
    "\n",
    "        Args:\n",
    "            rewards: deque\n",
    "            scores_eval: list\n",
    "        \"\"\"\n",
    "        self.avg_reward = np.mean(rewards)\n",
    "        self.max_reward = np.max(rewards)\n",
    "        self.std_reward = np.sqrt(np.var(rewards) / len(rewards))\n",
    "\n",
    "        if len(scores_eval) > 0:\n",
    "            self.eval_reward = scores_eval[-1]\n",
    "\n",
    "    def record_summary(self, t):\n",
    "        pass\n",
    "\n",
    "    def sample_path(self, env, num_episodes=None):\n",
    "        \"\"\"\n",
    "        Sample paths (trajectories) from the environment.\n",
    "\n",
    "        Args:\n",
    "            num_episodes: the number of episodes to be sampled\n",
    "                if none, sample one batch (size indicated by config file)\n",
    "            env: open AI Gym envinronment\n",
    "\n",
    "        Returns:\n",
    "            paths: a list of paths. Each path in paths is a dictionary with\n",
    "                path[\"observation\"] a numpy array of ordered observations in the path\n",
    "                path[\"actions\"] a numpy array of the corresponding actions in the path\n",
    "                path[\"reward\"] a numpy array of the corresponding rewards in the path\n",
    "            total_rewards: the sum of all rewards encountered during this \"path\"\n",
    "\n",
    "        You do not have to implement anything in this function, but you will need to\n",
    "        understand what it returns, and it is worthwhile to look over the code\n",
    "        just so you understand how we are taking actions in the environment\n",
    "        and generating batches to train on.\n",
    "        \"\"\"\n",
    "        episode = 0\n",
    "        episode_rewards = []\n",
    "        paths = []\n",
    "        t = 0\n",
    "\n",
    "        while num_episodes or t < self.config.batch_size:\n",
    "            state = env.reset()\n",
    "            states, actions, rewards = [], [], []\n",
    "            episode_reward = 0\n",
    "\n",
    "            for step in range(self.config.max_ep_len):\n",
    "                states.append(state)\n",
    "                action = self.policy.act(states[-1][None])[0]\n",
    "                state, reward, done, info = env.step(action)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                t += 1\n",
    "                if done or step == self.config.max_ep_len - 1:\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    break\n",
    "                if (not num_episodes) and t == self.config.batch_size:\n",
    "                    break\n",
    "\n",
    "            path = {\n",
    "                \"observation\": np.array(states),\n",
    "                \"reward\": np.array(rewards),\n",
    "                \"action\": np.array(actions),\n",
    "            }\n",
    "            paths.append(path)\n",
    "            episode += 1\n",
    "            if num_episodes and episode >= num_episodes:\n",
    "                break\n",
    "\n",
    "        return paths, episode_rewards\n",
    "\n",
    "    def get_returns(self, paths):\n",
    "        \"\"\"\n",
    "        Calculate the returns G_t for each timestep\n",
    "\n",
    "        Args:\n",
    "            paths: recorded sample paths. See sample_path() for details.\n",
    "\n",
    "        Return:\n",
    "            returns: return G_t for each timestep\n",
    "\n",
    "        After acting in the environment, we record the observations, actions, and\n",
    "        rewards. To get the advantages that we need for the policy update, we have\n",
    "        to convert the rewards into returns, G_t, which are themselves an estimate\n",
    "        of Q^π (s_t, a_t):\n",
    "\n",
    "           G_t = r_t + γ r_{t+1} + γ^2 r_{t+2} + ... + γ^{T-t} r_T\n",
    "\n",
    "        where T is the last timestep of the episode.\n",
    "\n",
    "        Note that here we are creating a list of returns for each path\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        all_returns = []\n",
    "        for path in paths:\n",
    "            rewards = path[\"reward\"]\n",
    "            returns = []\n",
    "            g_t = 0\n",
    "            for r in np.flip(rewards, 0):\n",
    "                # TODO check correctness\n",
    "                g_t = r + self.config.gamma*g_t\n",
    "                returns.append(g_t)\n",
    "            returns.reverse()\n",
    "            all_returns.append(returns)\n",
    "        returns = np.concatenate(all_returns)\n",
    "\n",
    "        return returns\n",
    "\n",
    "    def normalize_advantage(self, advantages):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            advantages: np.array of shape [batch size]\n",
    "        Returns:\n",
    "            normalized_advantages: np.array of shape [batch size]\n",
    "\n",
    "        Normalize the advantages so that they have a mean of 0 and standard\n",
    "        deviation of 1. Put the result in a variable called\n",
    "        normalized_advantages (which will be returned).\n",
    "\n",
    "        Note:\n",
    "        This function is called only if self.config.normalize_advantage is True.\n",
    "        \"\"\"\n",
    "        advantages -= np.mean(advantages, axis=-1)\n",
    "        advantages /= np.std(advantages, axis=-1)\n",
    "        return advantages\n",
    "\n",
    "    def calculate_advantage(self, returns, observations):\n",
    "        \"\"\"\n",
    "        Calculates the advantage for each of the observations\n",
    "        Args:\n",
    "            returns: np.array of shape [batch size]\n",
    "            observations: np.array of shape [batch size, dim(observation space)]\n",
    "        Returns:\n",
    "            advantages: np.array of shape [batch size]\n",
    "        \"\"\"\n",
    "        if self.config.use_baseline:\n",
    "            # override the behavior of advantage by subtracting baseline\n",
    "            advantages = self.baseline_network.calculate_advantage(\n",
    "                returns, observations\n",
    "            )\n",
    "        else:\n",
    "            advantages = returns\n",
    "\n",
    "        if self.config.normalize_advantage:\n",
    "            advantages = self.normalize_advantage(advantages)\n",
    "\n",
    "        return advantages\n",
    "\n",
    "    def update_policy(self, observations, actions, advantages):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            observations: np.array of shape [batch size, dim(observation space)]\n",
    "            actions: np.array of shape\n",
    "                [batch size, dim(action space)] if continuous\n",
    "                [batch size] (and integer type) if discrete\n",
    "            advantages: np.array of shape [batch size]\n",
    "\n",
    "        Perform one update on the policy using the provided data.\n",
    "        To compute the loss, you will need the log probabilities of the actions\n",
    "        given the observations. Note that the policy's action_distribution\n",
    "        method returns an instance of a subclass of\n",
    "        torch.distributions.Distribution, and that object can be used to\n",
    "        compute log probabilities.\n",
    "        See https://pytorch.org/docs/stable/distributions.html#distribution\n",
    "\n",
    "        Note:\n",
    "        PyTorch optimizers will try to minimize the loss you compute, but you\n",
    "        want to maximize the policy's performance.\n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            log_probs = self.policy.action_distribution(observations).log_prob(actions)\n",
    "            loss = log_probs * advantages\n",
    "            loss = -tf.math.reduce_mean(loss)\n",
    "        grads = tape.gradient(loss, self.policy.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.policy.trainable_weights))\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Performs training\n",
    "\n",
    "        You do not have to change or use anything here, but take a look\n",
    "        to see how all the code you've written fits together!\n",
    "        \"\"\"\n",
    "        last_record = 0\n",
    "\n",
    "        self.init_averages()\n",
    "        all_total_rewards = (\n",
    "            []\n",
    "        )  # the returns of all episodes samples for training purposes\n",
    "        averaged_total_rewards = []  # the returns for each iteration\n",
    "\n",
    "        for t in range(self.config.num_batches):\n",
    "            # collect a minibatch of samples\n",
    "            paths, total_rewards = self.sample_path(self.env)\n",
    "            all_total_rewards.extend(total_rewards)\n",
    "            observations = np.concatenate([path[\"observation\"] for path in paths])\n",
    "            actions = np.concatenate([path[\"action\"] for path in paths])\n",
    "            rewards = np.concatenate([path[\"reward\"] for path in paths])\n",
    "            # compute Q-val estimates (discounted future returns) for each time step\n",
    "            returns = self.get_returns(paths)\n",
    "\n",
    "            # advantage will depend on the baseline implementation\n",
    "            advantages = self.calculate_advantage(returns, observations)\n",
    "\n",
    "            # run training operations\n",
    "            if self.config.use_baseline:\n",
    "                self.baseline_network.update_baseline(returns, observations)\n",
    "            self.update_policy(observations, actions, advantages)\n",
    "\n",
    "            # logging\n",
    "            if t % self.config.summary_freq == 0:\n",
    "                self.update_averages(total_rewards, all_total_rewards)\n",
    "                self.record_summary(t)\n",
    "\n",
    "            # compute reward statistics for this batch and log\n",
    "            avg_reward = np.mean(total_rewards)\n",
    "            sigma_reward = np.sqrt(np.var(total_rewards) / len(total_rewards))\n",
    "            msg = \"Average reward: {:04.2f} +/- {:04.2f}\".format(\n",
    "                avg_reward, sigma_reward\n",
    "            )\n",
    "            averaged_total_rewards.append(avg_reward)\n",
    "            self.logger.info(msg)\n",
    "\n",
    "            if self.config.record and (last_record > self.config.record_freq):\n",
    "                self.logger.info(\"Recording...\")\n",
    "                last_record = 0\n",
    "                self.record()\n",
    "\n",
    "        self.logger.info(\"- Training done.\")\n",
    "        np.save(self.config.scores_output, averaged_total_rewards)\n",
    "        export_plot(\n",
    "            averaged_total_rewards,\n",
    "            \"Score\",\n",
    "            self.config.env_name,\n",
    "            self.config.plot_output,\n",
    "        )\n",
    "\n",
    "    def evaluate(self, env=None, num_episodes=1):\n",
    "        \"\"\"\n",
    "        Evaluates the return for num_episodes episodes.\n",
    "        Not used right now, all evaluation statistics are computed during training\n",
    "        episodes.\n",
    "        \"\"\"\n",
    "        if env == None:\n",
    "            env = self.env\n",
    "        paths, rewards = self.sample_path(env, num_episodes)\n",
    "        avg_reward = np.mean(rewards)\n",
    "        sigma_reward = np.sqrt(np.var(rewards) / len(rewards))\n",
    "        msg = \"Average reward: {:04.2f} +/- {:04.2f}\".format(avg_reward, sigma_reward)\n",
    "        self.logger.info(msg)\n",
    "        return avg_reward\n",
    "\n",
    "    def record(self):\n",
    "        \"\"\"\n",
    "        Recreate an env and record a video for one episode\n",
    "        \"\"\"\n",
    "        env = gym.make(self.config.env_name)\n",
    "        env.seed(self.seed)\n",
    "        env = gym.wrappers.Monitor(\n",
    "            env, self.config.record_path, video_callable=lambda x: True, resume=True\n",
    "        )\n",
    "        self.evaluate(env, 1)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Apply procedures of training for a PG.\n",
    "        \"\"\"\n",
    "        # record one game at the beginning\n",
    "        if self.config.record:\n",
    "            self.record()\n",
    "        # model\n",
    "        self.train()\n",
    "        # record one game at the end\n",
    "        if self.config.record:\n",
    "            self.record()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
